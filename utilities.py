"""
üîß EDA Utilities - Pipeline de An√°lisis Exploratorio de Datos
===========================================================

Funciones robustas y reutilizables para an√°lisis univariado, multivariado 
y discretizaci√≥n de variables basadas en el pipeline del dataset Titanic.

"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats
from scipy.stats import (
    skew, kurtosis, entropy, shapiro, normaltest, 
    spearmanr, pearsonr, kendalltau, ttest_ind, 
    f_oneway, mannwhitneyu, kruskal, chi2_contingency
)
from sklearn.preprocessing import KBinsDiscretizer
from sklearn.tree import DecisionTreeClassifier
import warnings
warnings.filterwarnings('ignore')

# ============================================================================
# üìä AN√ÅLISIS UNIVARIADO - VARIABLES CUANTITATIVAS
# ============================================================================

def analisis_cuantitativo_completo(df, columnas=None, figsize=(15, 10)):
    """
    An√°lisis univariado completo para variables cuantitativas.
    
    Parameters:
    -----------
    df : pd.DataFrame
        DataFrame con los datos
    columnas : list, optional
        Lista de columnas a analizar. Si None, analiza todas las num√©ricas
    figsize : tuple
        Tama√±o de las figuras
    
    Returns:
    --------
    pd.DataFrame : Resumen estad√≠stico completo
    """
    if columnas is None:
        columnas = df.select_dtypes(include=[np.number]).columns.tolist()
    
    resultados = []
    
    for col in columnas:
        serie = df[col].dropna()
        if len(serie) == 0:
            continue
            
        # M√©tricas b√°sicas
        media = serie.mean()
        mediana = serie.median()
        moda = serie.mode().iloc[0] if not serie.mode().empty else np.nan
        
        # Dispersi√≥n
        desv_std = serie.std()
        varianza = serie.var()
        rango = serie.max() - serie.min()
        iqr = serie.quantile(0.75) - serie.quantile(0.25)
        mad = np.median(np.abs(serie - mediana))
        cv = (desv_std / media * 100) if media != 0 else np.nan
        
        # Forma de distribuci√≥n
        asimetria = skew(serie, bias=False)
        curtosis_val = kurtosis(serie, fisher=True, bias=False)
        
        # Pruebas de normalidad
        if len(serie) <= 5000:
            _, p_shapiro = shapiro(serie.sample(min(len(serie), 5000), random_state=42))
        else:
            p_shapiro = np.nan
        _, p_dagostino = normaltest(serie)
        
        # Outliers (m√©todo IQR)
        q1, q3 = serie.quantile([0.25, 0.75])
        limite_inf = q1 - 1.5 * iqr
        limite_sup = q3 + 1.5 * iqr
        outliers = ((serie < limite_inf) | (serie > limite_sup)).sum()
        
        resultados.append({
            'Variable': col,
            'N_v√°lidos': len(serie),
            'N_missing': df[col].isna().sum(),
            'Media': media,
            'Mediana': mediana,
            'Moda': moda,
            'Desv_Std': desv_std,
            'Varianza': varianza,
            'Rango': rango,
            'IQR': iqr,
            'MAD': mad,
            'CV_%': cv,
            'Asimetr√≠a': asimetria,
            'Curtosis': curtosis_val,
            'Outliers_IQR': outliers,
            'p_Shapiro': p_shapiro,
            'p_DAgostino': p_dagostino,
            'Normal_Shapiro': 'S√≠' if p_shapiro > 0.05 else 'No' if not pd.isna(p_shapiro) else 'N/A',
            'Normal_DAgostino': 'S√≠' if p_dagostino > 0.05 else 'No'
        })
    
    return pd.DataFrame(resultados).round(4)

def visualizar_cuantitativas(df, columnas=None, figsize=(15, 12)):
    """
    Visualizaci√≥n completa para variables cuantitativas.
    
    Parameters:
    -----------
    df : pd.DataFrame
        DataFrame con los datos
    columnas : list, optional
        Lista de columnas a analizar
    figsize : tuple
        Tama√±o de la figura
    """
    if columnas is None:
        columnas = df.select_dtypes(include=[np.number]).columns.tolist()
    
    n_cols = len(columnas)
    n_rows = (n_cols + 2) // 3  # 3 columnas por fila
    
    fig, axes = plt.subplots(n_rows, 3, figsize=figsize)
    if n_rows == 1:
        axes = axes.reshape(1, -1)
    axes = axes.flatten()
    
    for i, col in enumerate(columnas):
        serie = df[col].dropna()
        
        # Histograma + KDE
        sns.histplot(serie, kde=True, ax=axes[i], alpha=0.7)
        axes[i].axvline(serie.mean(), color='red', linestyle='--', alpha=0.8, label=f'Media: {serie.mean():.2f}')
        axes[i].axvline(serie.median(), color='green', linestyle='--', alpha=0.8, label=f'Mediana: {serie.median():.2f}')
        axes[i].set_title(f'{col}\nAsimetr√≠a: {skew(serie, bias=False):.3f} | Curtosis: {kurtosis(serie, fisher=True, bias=False):.3f}')
        axes[i].legend(fontsize=8)
    
    # Ocultar subplots vac√≠os
    for j in range(i + 1, len(axes)):
        axes[j].set_visible(False)
    
    plt.tight_layout()
    plt.show()
    
    # Boxplots separados
    if len(columnas) > 1:
        fig, ax = plt.subplots(figsize=(max(8, len(columnas) * 1.5), 6))
        df[columnas].boxplot(ax=ax)
        ax.set_title('Distribuci√≥n y Outliers - Boxplots')
        plt.xticks(rotation=45)
        plt.tight_layout()
        plt.show()

# ============================================================================
# üìä AN√ÅLISIS UNIVARIADO - VARIABLES CUALITATIVAS
# ============================================================================

def analisis_cualitativo_completo(df, columnas=None):
    """
    An√°lisis univariado completo para variables cualitativas.
    
    Parameters:
    -----------
    df : pd.DataFrame
        DataFrame con los datos
    columnas : list, optional
        Lista de columnas categ√≥ricas a analizar
        
    Returns:
    --------
    dict : Diccionario con res√∫menes por variable
    """
    if columnas is None:
        columnas = df.select_dtypes(include=['object', 'category']).columns.tolist()
    
    resultados = {}
    
    for col in columnas:
        serie = df[col].dropna()
        if len(serie) == 0:
            continue
            
        # Frecuencias
        freq_abs = serie.value_counts()
        freq_rel = serie.value_counts(normalize=True)
        
        # Diversidad
        k = len(freq_rel)  # n√∫mero de categor√≠as
        H = entropy(freq_rel, base=2)  # entrop√≠a Shannon
        H_max = np.log2(k) if k > 1 else 0
        G = 1 - (freq_rel**2).sum()  # √≠ndice Gini
        G_max = 1 - 1/k if k > 1 else 0
        
        resultados[col] = {
            'n_valid': len(serie),
            'n_missing': df[col].isna().sum(),
            'n_categories': k,
            'moda': freq_abs.index[0],
            'freq_moda': freq_abs.iloc[0],
            'prop_moda': freq_rel.iloc[0],
            'entropia_bits': H,
            'entropia_max': H_max,
            'entropia_norm': H / H_max if H_max > 0 else 0,
            'gini': G,
            'gini_max': G_max,
            'gini_norm': G / G_max if G_max > 0 else 0,
            'freq_abs': freq_abs,
            'freq_rel': freq_rel
        }
    
    return resultados

def visualizar_cualitativas(df, columnas=None, figsize=(15, 10), max_categories=10):
    """
    Visualizaci√≥n para variables cualitativas.
    
    Parameters:
    -----------
    df : pd.DataFrame
        DataFrame con los datos
    columnas : list, optional
        Lista de columnas categ√≥ricas
    figsize : tuple
        Tama√±o de la figura
    max_categories : int
        M√°ximo n√∫mero de categor√≠as a mostrar (agrupa el resto en "Otros")
    """
    if columnas is None:
        columnas = df.select_dtypes(include=['object', 'category']).columns.tolist()
    
    n_cols = len(columnas)
    n_rows = (n_cols + 1) // 2  # 2 columnas por fila
    
    fig, axes = plt.subplots(n_rows, 2, figsize=figsize)
    if n_rows == 1:
        axes = axes.reshape(1, -1)
    axes = axes.flatten()
    
    for i, col in enumerate(columnas):
        serie = df[col].dropna()
        freq = serie.value_counts()
        
        # Agrupar categor√≠as raras en "Otros"
        if len(freq) > max_categories:
            top_cats = freq.head(max_categories - 1)
            otros = freq.tail(len(freq) - max_categories + 1).sum()
            freq_plot = pd.concat([top_cats, pd.Series({'Otros': otros})])
        else:
            freq_plot = freq
        
        # Gr√°fico de barras
        freq_plot.plot(kind='bar', ax=axes[i], alpha=0.8)
        axes[i].set_title(f'{col}\nCategor√≠as: {len(freq)} | Entrop√≠a: {entropy(serie.value_counts(normalize=True), base=2):.2f} bits')
        axes[i].set_xlabel('Categor√≠as')
        axes[i].set_ylabel('Frecuencia')
        axes[i].tick_params(axis='x', rotation=45)
    
    # Ocultar subplots vac√≠os
    for j in range(i + 1, len(axes)):
        axes[j].set_visible(False)
    
    plt.tight_layout()
    plt.show()

# ============================================================================
# üîó AN√ÅLISIS MULTIVARIADO
# ============================================================================

def correlaciones_completas(df, columnas=None, metodos=['pearson', 'spearman', 'kendall']):
    """
    An√°lisis completo de correlaciones con p-valores.
    
    Parameters:
    -----------
    df : pd.DataFrame
        DataFrame con los datos
    columnas : list, optional
        Lista de columnas num√©ricas a analizar
    metodos : list
        M√©todos de correlaci√≥n a calcular
        
    Returns:
    --------
    dict : Diccionario con matrices de correlaci√≥n y p-valores
    """
    if columnas is None:
        columnas = df.select_dtypes(include=[np.number]).columns.tolist()
    
    df_num = df[columnas].copy()
    resultados = {}
    
    for metodo in metodos:
        if metodo == 'pearson':
            corr_func = pearsonr
        elif metodo == 'spearman':
            corr_func = spearmanr
        elif metodo == 'kendall':
            corr_func = kendalltau
        else:
            continue
            
        # Matriz de correlaciones
        corr_matrix = df_num.corr(method=metodo)
        
        # Matriz de p-valores
        n_vars = len(columnas)
        p_matrix = np.zeros((n_vars, n_vars))
        
        for i in range(n_vars):
            for j in range(n_vars):
                if i == j:
                    p_matrix[i, j] = 0
                else:
                    # Eliminar NaN para el c√°lculo
                    col1, col2 = columnas[i], columnas[j]
                    common_idx = df_num[[col1, col2]].dropna().index
                    if len(common_idx) > 2:
                        _, p_val = corr_func(df_num.loc[common_idx, col1], 
                                           df_num.loc[common_idx, col2])
                        p_matrix[i, j] = p_val
                    else:
                        p_matrix[i, j] = np.nan
        
        p_values = pd.DataFrame(p_matrix, index=columnas, columns=columnas)
        
        resultados[metodo] = {
            'correlaciones': corr_matrix,
            'p_valores': p_values
        }
    
    return resultados

def resumen_correlaciones(df, columnas=None, metodo='spearman', alpha=0.05):
    """
    Resumen de correlaciones por pares con interpretaci√≥n.
    
    Parameters:
    -----------
    df : pd.DataFrame
        DataFrame con los datos
    columnas : list, optional
        Lista de columnas num√©ricas
    metodo : str
        M√©todo de correlaci√≥n ('pearson', 'spearman', 'kendall')
    alpha : float
        Nivel de significancia
        
    Returns:
    --------
    pd.DataFrame : Resumen de correlaciones por pares
    """
    correlaciones = correlaciones_completas(df, columnas, [metodo])
    corr_matrix = correlaciones[metodo]['correlaciones']
    p_matrix = correlaciones[metodo]['p_valores']
    
    # Crear lista de pares √∫nicos
    pairs_data = []
    cols = corr_matrix.columns
    
    for i in range(len(cols)):
        for j in range(i+1, len(cols)):
            var1, var2 = cols[i], cols[j]
            corr = corr_matrix.loc[var1, var2]
            p_val = p_matrix.loc[var1, var2]
            
            # Clasificar fuerza
            abs_corr = abs(corr)
            if abs_corr < 0.1:
                fuerza = "Muy d√©bil"
            elif abs_corr < 0.3:
                fuerza = "D√©bil"
            elif abs_corr < 0.5:
                fuerza = "Moderada"
            elif abs_corr < 0.7:
                fuerza = "Fuerte"
            else:
                fuerza = "Muy fuerte"
                
            pairs_data.append({
                'Par_Variables': f"{var1} ‚Üî {var2}",
                f'{metodo.capitalize()}_œÅ': corr,
                'p-valor': p_val,
                f'Significativo_Œ±{str(alpha).replace("0.", "")}': 'S√≠' if p_val < alpha else 'No',
                'Fuerza': fuerza
            })
    
    # Crear DataFrame y ordenar por p-valor
    df_resultado = pd.DataFrame(pairs_data)
    df_resultado = df_resultado.sort_values('p-valor')
    df_resultado[f'{metodo.capitalize()}_œÅ'] = df_resultado[f'{metodo.capitalize()}_œÅ'].round(4)
    df_resultado['p-valor'] = df_resultado['p-valor'].round(6)
    
    return df_resultado

def visualizar_correlaciones(df, columnas=None, metodo='spearman', figsize=(16, 12)):
    """
    Visualizaci√≥n completa de correlaciones.
    
    Parameters:
    -----------
    df : pd.DataFrame
        DataFrame con los datos
    columnas : list, optional
        Lista de columnas num√©ricas
    metodo : str
        M√©todo de correlaci√≥n
    figsize : tuple
        Tama√±o de la figura
    """
    correlaciones = correlaciones_completas(df, columnas, [metodo])
    corr_matrix = correlaciones[metodo]['correlaciones']
    p_matrix = correlaciones[metodo]['p_valores']
    
    fig, axes = plt.subplots(1, 3, figsize=figsize)
    
    # Heatmap 1: Correlaciones
    sns.heatmap(corr_matrix, annot=True, fmt=".3f", cmap="coolwarm", 
                center=0, ax=axes[0], cbar_kws={'label': f'Correlaci√≥n {metodo}'})
    axes[0].set_title(f'Correlaciones de {metodo.capitalize()}', fontweight='bold')
    
    # Heatmap 2: P-valores
    sns.heatmap(p_matrix, annot=True, fmt=".3f", cmap="Reds_r", 
                ax=axes[1], cbar_kws={'label': 'p-valor'})
    axes[1].set_title('P-valores', fontweight='bold')
    
    # Heatmap 3: Solo significativas
    mask_nonsig = p_matrix > 0.05
    sns.heatmap(corr_matrix, annot=True, fmt=".3f", cmap="coolwarm", 
                center=0, mask=mask_nonsig, ax=axes[2],
                cbar_kws={'label': f'Correlaci√≥n {metodo} (p < 0.05)'})
    axes[2].set_title('Solo Correlaciones Significativas', fontweight='bold')
    
    plt.tight_layout()
    plt.show()

# ============================================================================
# üîÑ COMPARACI√ìN GRUPOS (CUANTI vs CUALI)
# ============================================================================

def comparar_grupos_cuanti(df, var_continua, var_categorica, alpha=0.05):
    """
    Comparaci√≥n de grupos para variable continua vs categ√≥rica.
    
    Parameters:
    -----------
    df : pd.DataFrame
        DataFrame con los datos
    var_continua : str
        Nombre de la variable continua
    var_categorica : str
        Nombre de la variable categ√≥rica
    alpha : float
        Nivel de significancia
        
    Returns:
    --------
    dict : Resultados de las pruebas estad√≠sticas
    """
    # Preparar datos
    data_clean = df[[var_continua, var_categorica]].dropna()
    grupos = data_clean[var_categorica].unique()
    n_grupos = len(grupos)
    
    if n_grupos < 2:
        return {"error": "Se necesitan al menos 2 grupos"}
    
    resultados = {
        'variable_continua': var_continua,
        'variable_categorica': var_categorica,
        'n_grupos': n_grupos,
        'grupos': grupos.tolist()
    }
    
    # Separar datos por grupo
    grupos_data = []
    for grupo in grupos:
        grupo_datos = data_clean[data_clean[var_categorica] == grupo][var_continua].values
        grupos_data.append(grupo_datos)
        
        # Estad√≠sticas descriptivas por grupo
        resultados[f'media_{grupo}'] = np.mean(grupo_datos)
        resultados[f'mediana_{grupo}'] = np.median(grupo_datos)
        resultados[f'n_{grupo}'] = len(grupo_datos)
    
    # Funci√≥n para Cohen's d
    def cohens_d(x, y):
        n1, n2 = len(x), len(y)
        s1, s2 = np.var(x, ddof=1), np.var(y, ddof=1)
        s_p = np.sqrt(((n1-1)*s1 + (n2-1)*s2) / (n1+n2-2))
        return (np.mean(x) - np.mean(y)) / s_p
    
    if n_grupos == 2:
        # Pruebas para 2 grupos
        grupo1_data, grupo2_data = grupos_data[0], grupos_data[1]
        
        # T-test (Welch)
        stat_t, p_t = ttest_ind(grupo1_data, grupo2_data, equal_var=False)
        d_cohen = cohens_d(grupo1_data, grupo2_data)
        
        # Mann-Whitney U
        stat_mw, p_mw = mannwhitneyu(grupo1_data, grupo2_data, alternative='two-sided')
        
        resultados.update({
            'test_t_stat': stat_t,
            'test_t_pvalue': p_t,
            'test_t_significativo': p_t < alpha,
            'cohens_d': d_cohen,
            'mann_whitney_stat': stat_mw,
            'mann_whitney_pvalue': p_mw,
            'mann_whitney_significativo': p_mw < alpha
        })
        
    else:
        # Pruebas para k > 2 grupos
        # ANOVA
        stat_f, p_f = f_oneway(*grupos_data)
        
        # Eta cuadrado
        k = len(grupos_data)
        n = sum(len(g) for g in grupos_data)
        eta2 = (stat_f * (k - 1)) / (stat_f * (k - 1) + (n - k))
        
        # Kruskal-Wallis
        stat_kw, p_kw = kruskal(*grupos_data)
        
        resultados.update({
            'anova_f_stat': stat_f,
            'anova_pvalue': p_f,
            'anova_significativo': p_f < alpha,
            'eta_cuadrado': eta2,
            'kruskal_wallis_stat': stat_kw,
            'kruskal_wallis_pvalue': p_kw,
            'kruskal_wallis_significativo': p_kw < alpha
        })
    
    return resultados

def visualizar_grupos_cuanti(df, var_continua, var_categorica, figsize=(15, 5)):
    """
    Visualizaci√≥n de comparaci√≥n de grupos.
    
    Parameters:
    -----------
    df : pd.DataFrame
        DataFrame con los datos
    var_continua : str
        Variable continua
    var_categorica : str
        Variable categ√≥rica
    figsize : tuple
        Tama√±o de la figura
    """
    fig, axes = plt.subplots(1, 3, figsize=figsize)
    
    # Boxplot
    sns.boxplot(data=df, x=var_categorica, y=var_continua, ax=axes[0])
    axes[0].set_title('Distribuci√≥n por Grupo')
    axes[0].tick_params(axis='x', rotation=45)
    
    # Histogramas superpuestos
    for grupo in df[var_categorica].dropna().unique():
        datos_grupo = df[df[var_categorica] == grupo][var_continua].dropna()
        axes[1].hist(datos_grupo, alpha=0.7, label=f'{grupo} (n={len(datos_grupo)})', density=True)
    axes[1].set_xlabel(var_continua)
    axes[1].set_ylabel('Densidad')
    axes[1].set_title('Distribuciones Superpuestas')
    axes[1].legend()
    
    # Violin plot
    sns.violinplot(data=df, x=var_categorica, y=var_continua, ax=axes[2])
    axes[2].set_title('Densidad por Grupo')
    axes[2].tick_params(axis='x', rotation=45)
    
    plt.tight_layout()
    plt.show()

# ============================================================================
# üîó AN√ÅLISIS CATEG√ìRICO vs CATEG√ìRICO
# ============================================================================

def analizar_independencia_categoricas(df, var1, var2, alpha=0.05):
    """
    An√°lisis de independencia entre dos variables categ√≥ricas.
    
    Parameters:
    -----------
    df : pd.DataFrame
        DataFrame con los datos
    var1, var2 : str
        Nombres de las variables categ√≥ricas
    alpha : float
        Nivel de significancia
        
    Returns:
    --------
    dict : Resultados del an√°lisis de independencia
    """
    # Tabla de contingencia
    tabla_contingencia = pd.crosstab(df[var1], df[var2])
    
    # Chi-cuadrado
    chi2, p_valor, grados_libertad, frecuencias_esperadas = chi2_contingency(tabla_contingencia)
    
    # Cram√©r's V
    n = tabla_contingencia.sum().sum()
    r, k = tabla_contingencia.shape
    phi2 = chi2 / n
    cramers_v = np.sqrt(phi2 / min(r-1, k-1)) if min(r-1, k-1) > 0 else np.nan
    
    # Cram√©r's V corregido (bias correction)
    phi2_corr = max(0, phi2 - (k-1)*(r-1)/(n-1))
    denom_corr = min(k-1, r-1) - (k-1)*(r-1)/(n-1)
    cramers_v_corregido = np.sqrt(phi2_corr / denom_corr) if denom_corr > 0 else np.nan
    
    # Clasificar fuerza de asociaci√≥n
    if cramers_v < 0.1:
        fuerza = "Muy d√©bil"
    elif cramers_v < 0.3:
        fuerza = "Moderada"
    elif cramers_v < 0.5:
        fuerza = "Fuerte"
    else:
        fuerza = "Muy fuerte"
    
    # Porcentajes por fila
    porcentajes_fila = (tabla_contingencia.div(tabla_contingencia.sum(axis=1), axis=0) * 100).round(1)
    
    resultados = {
        'variables': f"{var1} ‚Üî {var2}",
        'tabla_contingencia': tabla_contingencia,
        'porcentajes_fila': porcentajes_fila,
        'chi2_estadistico': chi2,
        'p_valor': p_valor,
        'grados_libertad': grados_libertad,
        'cramers_v': cramers_v,
        'cramers_v_corregido': cramers_v_corregido,
        'significativo': p_valor < alpha,
        'fuerza_asociacion': fuerza,
        'n_total': n,
        'frecuencias_esperadas': pd.DataFrame(frecuencias_esperadas, 
                                            index=tabla_contingencia.index,
                                            columns=tabla_contingencia.columns)
    }
    
    return resultados

def visualizar_categoricas(df, var1, var2, figsize=(15, 5)):
    """
    Visualizaci√≥n para variables categ√≥ricas.
    
    Parameters:
    -----------
    df : pd.DataFrame
        DataFrame con los datos
    var1, var2 : str
        Variables categ√≥ricas
    figsize : tuple
        Tama√±o de la figura
    """
    fig, axes = plt.subplots(1, 3, figsize=figsize)
    
    # Tabla de contingencia como heatmap
    tabla = pd.crosstab(df[var1], df[var2])
    sns.heatmap(tabla, annot=True, fmt='d', cmap='Blues', ax=axes[0])
    axes[0].set_title('Tabla de Contingencia')
    
    # Gr√°fico de barras agrupadas
    tabla.plot(kind='bar', ax=axes[1], alpha=0.8)
    axes[1].set_title('Frecuencias por Categor√≠a')
    axes[1].set_xlabel(var1)
    axes[1].set_ylabel('Frecuencia')
    axes[1].legend(title=var2)
    axes[1].tick_params(axis='x', rotation=45)
    
    # Porcentajes por fila
    porcentajes = (tabla.div(tabla.sum(axis=1), axis=0) * 100)
    sns.heatmap(porcentajes, annot=True, fmt='.1f', cmap='Oranges', ax=axes[2])
    axes[2].set_title('Porcentajes por Fila (%)')
    
    plt.tight_layout()
    plt.show()

# ============================================================================
# üîÑ DISCRETIZACI√ìN DE VARIABLES
# ============================================================================

def discretizar_variable(df, columna, metodos=['equal_width', 'quantiles', 'kmeans', 'supervised'], 
                        n_bins=5, target=None, **kwargs):
    """
    Discretizaci√≥n de una variable con m√∫ltiples m√©todos.
    
    Parameters:
    -----------
    df : pd.DataFrame
        DataFrame con los datos
    columna : str
        Nombre de la columna a discretizar
    metodos : list
        Lista de m√©todos a aplicar
    n_bins : int
        N√∫mero de bins
    target : str, optional
        Variable objetivo para m√©todo supervisado
    **kwargs : dict
        Argumentos adicionales para DecisionTreeClassifier
        
    Returns:
    --------
    dict : Resultados de discretizaci√≥n por m√©todo
    """
    serie = df[columna].dropna()
    if len(serie) == 0:
        return {"error": "Serie vac√≠a"}
    
    resultados = {
        'variable': columna,
        'n_valores': len(serie),
        'metodos': {}
    }
    
    for metodo in metodos:
        if metodo == 'equal_width':
            # Intervalos de igual ancho
            bins = pd.cut(serie, bins=n_bins, include_lowest=True)
            edges = pd.cut(serie, bins=n_bins, include_lowest=True, retbins=True)[1]
            
            resultados['metodos']['equal_width'] = {
                'bins': bins,
                'edges': edges,
                'frecuencias': bins.value_counts().sort_index(),
                'descripcion': f"Intervalos de igual ancho ({n_bins} bins)"
            }
            
        elif metodo == 'quantiles':
            # Cuantiles (igual frecuencia)
            bins = pd.qcut(serie, q=n_bins, duplicates='drop')
            edges = pd.qcut(serie, q=n_bins, duplicates='drop', retbins=True)[1]
            
            resultados['metodos']['quantiles'] = {
                'bins': bins,
                'edges': edges,
                'frecuencias': bins.value_counts().sort_index(),
                'descripcion': f"Cuantiles - igual frecuencia ({n_bins} bins)"
            }
            
        elif metodo == 'kmeans':
            # K-Means clustering
            discretizer = KBinsDiscretizer(n_bins=n_bins, encode='ordinal', strategy='kmeans')
            bins_encoded = discretizer.fit_transform(serie.to_frame()).astype(int).ravel()
            edges = discretizer.bin_edges_[0]
            
            # Crear labels interpretables
            bin_labels = []
            for i in range(len(edges)-1):
                bin_labels.append(f"[{edges[i]:.2f}, {edges[i+1]:.2f})")
            
            resultados['metodos']['kmeans'] = {
                'bins_encoded': bins_encoded,
                'edges': edges,
                'bin_labels': bin_labels,
                'frecuencias': pd.Series(bins_encoded).value_counts().sort_index(),
                'descripcion': f"K-Means clustering ({n_bins} bins)"
            }
            
        elif metodo == 'supervised' and target is not None:
            # M√©todo supervisado con √°rboles de decisi√≥n
            mask = df[[columna, target]].dropna().index
            X = df.loc[mask, [columna]].values
            y = df.loc[mask, target].values
            
            # Par√°metros por defecto para el √°rbol
            tree_params = {
                'max_depth': 3,
                'min_samples_leaf': 50,
                'random_state': 42
            }
            tree_params.update(kwargs)
            
            clf = DecisionTreeClassifier(**tree_params)
            clf.fit(X, y)
            
            # Extraer umbrales de corte
            thresholds = sorted([t for t in clf.tree_.threshold if t != -2])
            
            # Crear bins usando los umbrales
            if thresholds:
                edges = [-np.inf] + thresholds + [np.inf]
                bins = pd.cut(serie, bins=edges, include_lowest=True)
                
                resultados['metodos']['supervised'] = {
                    'bins': bins,
                    'thresholds': thresholds,
                    'edges': edges,
                    'frecuencias': bins.value_counts().sort_index(),
                    'tree_params': tree_params,
                    'descripcion': f"Supervisado - Decision Tree (target: {target})"
                }
    
    return resultados

def visualizar_discretizacion(df, columna, resultados_discretizacion, figsize=(16, 12)):
    """
    Visualizaci√≥n de m√©todos de discretizaci√≥n.
    
    Parameters:
    -----------
    df : pd.DataFrame
        DataFrame con los datos
    columna : str
        Nombre de la columna
    resultados_discretizacion : dict
        Resultados de discretizar_variable()
    figsize : tuple
        Tama√±o de la figura
    """
    serie = df[columna].dropna()
    metodos = list(resultados_discretizacion['metodos'].keys())
    n_metodos = len(metodos)
    
    fig, axes = plt.subplots(2, n_metodos, figsize=figsize)
    if n_metodos == 1:
        axes = axes.reshape(-1, 1)
    
    fig.suptitle(f'Comparaci√≥n de M√©todos de Discretizaci√≥n - {columna}', fontsize=16, fontweight='bold')
    
    for i, metodo in enumerate(metodos):
        resultado = resultados_discretizacion['metodos'][metodo]
        
        # Fila 1: Histograma con l√≠neas de corte
        axes[0, i].hist(serie, bins=30, alpha=0.7, density=True, color='lightblue')
        
        if metodo in ['equal_width', 'quantiles']:
            edges = resultado['edges']
            for edge in edges[1:-1]:
                axes[0, i].axvline(edge, color='red', linestyle='--', alpha=0.8)
        elif metodo == 'kmeans':
            edges = resultado['edges']
            for edge in edges[1:-1]:
                axes[0, i].axvline(edge, color='purple', linestyle='--', alpha=0.8)
        elif metodo == 'supervised':
            thresholds = resultado['thresholds']
            for threshold in thresholds:
                axes[0, i].axvline(threshold, color='darkred', linestyle='--', alpha=0.8)
        
        axes[0, i].set_title(f'{metodo.replace("_", " ").title()}')
        axes[0, i].set_xlabel(columna)
        axes[0, i].set_ylabel('Densidad')
        
        # Fila 2: Frecuencias por bin
        frecuencias = resultado['frecuencias']
        if metodo == 'kmeans':
            axes[1, i].bar(range(len(frecuencias)), frecuencias.values)
            axes[1, i].set_xlabel('Bin ID')
            axes[1, i].set_xticks(range(len(frecuencias)))
        else:
            frecuencias.plot(kind='bar', ax=axes[1, i], alpha=0.8)
            axes[1, i].tick_params(axis='x', rotation=45)
        
        axes[1, i].set_ylabel('Frecuencia')
        axes[1, i].set_title('Distribuci√≥n de Frecuencias')
    
    plt.tight_layout()
    plt.show()

def comparar_discretizacion_supervivencia(df, columna, target, resultados_discretizacion):
    """
    An√°lisis de supervivencia por bins para m√©todos de discretizaci√≥n.
    
    Parameters:
    -----------
    df : pd.DataFrame
        DataFrame con los datos
    columna : str
        Columna discretizada
    target : str
        Variable objetivo (0/1)
    resultados_discretizacion : dict
        Resultados de discretizar_variable()
        
    Returns:
    --------
    pd.DataFrame : Tasas de supervivencia por m√©todo y bin
    """
    data_clean = df[[columna, target]].dropna()
    resultados_supervivencia = []
    
    for metodo, resultado in resultados_discretizacion['metodos'].items():
        if metodo in ['equal_width', 'quantiles', 'supervised']:
            bins = resultado['bins']
            
            # Alinear con datos limpios
            bins_aligned = bins.loc[data_clean.index]
            
            for bin_name in bins_aligned.unique():
                if pd.isna(bin_name):
                    continue
                    
                mask = bins_aligned == bin_name
                grupo_data = data_clean[mask]
                
                if len(grupo_data) > 0:
                    tasa_supervivencia = grupo_data[target].mean()
                    resultados_supervivencia.append({
                        'M√©todo': metodo,
                        'Bin': str(bin_name),
                        'N_pasajeros': len(grupo_data),
                        'Tasa_supervivencia': tasa_supervivencia,
                        'N_supervivientes': grupo_data[target].sum()
                    })
        
        elif metodo == 'kmeans':
            bins_encoded = resultado['bins_encoded']
            bin_labels = resultado['bin_labels']
            
            # Crear Series para facilitar an√°lisis
            bins_series = pd.Series(bins_encoded, index=data_clean.index)
            
            for bin_id in sorted(pd.Series(bins_encoded).unique()):
                mask = bins_series == bin_id
                grupo_data = data_clean[mask]
                
                if len(grupo_data) > 0:
                    tasa_supervivencia = grupo_data[target].mean()
                    bin_label = bin_labels[bin_id] if bin_id < len(bin_labels) else f"Bin {bin_id}"
                    
                    resultados_supervivencia.append({
                        'M√©todo': metodo,
                        'Bin': bin_label,
                        'N_pasajeros': len(grupo_data),
                        'Tasa_supervivencia': tasa_supervivencia,
                        'N_supervivientes': grupo_data[target].sum()
                    })
    
    df_supervivencia = pd.DataFrame(resultados_supervivencia)
    return df_supervivencia.round(3)

# ============================================================================
# üéØ FUNCIONES DE PIPELINE COMPLETO
# ============================================================================

def pipeline_eda_completo(df, target=None, figsize_univariado=(15, 12), figsize_multivariado=(16, 8)):
    """
    Pipeline completo de EDA.
    
    Parameters:
    -----------
    df : pd.DataFrame
        DataFrame con los datos
    target : str, optional
        Variable objetivo para an√°lisis supervisado
    figsize_univariado, figsize_multivariado : tuple
        Tama√±os de figuras
        
    Returns:
    --------
    dict : Resultados completos del EDA
    """
    print("üîç INICIANDO PIPELINE EDA COMPLETO")
    print("=" * 60)
    
    # Informaci√≥n general del dataset
    print(f"\nüìä INFORMACI√ìN GENERAL DEL DATASET")
    print(f"Dimensiones: {df.shape}")
    print(f"Variables num√©ricas: {len(df.select_dtypes(include=[np.number]).columns)}")
    print(f"Variables categ√≥ricas: {len(df.select_dtypes(include=['object', 'category']).columns)}")
    print(f"Valores faltantes: {df.isnull().sum().sum()}")
    
    resultados = {
        'info_general': {
            'dimensiones': df.shape,
            'n_numericas': len(df.select_dtypes(include=[np.number]).columns),
            'n_categoricas': len(df.select_dtypes(include=['object', 'category']).columns),
            'valores_faltantes': df.isnull().sum().sum()
        }
    }
    
    # 1. AN√ÅLISIS UNIVARIADO - CUANTITATIVAS
    print(f"\nüìà AN√ÅLISIS UNIVARIADO - VARIABLES CUANTITATIVAS")
    print("-" * 50)
    cols_numericas = df.select_dtypes(include=[np.number]).columns.tolist()
    
    if target and target in cols_numericas:
        cols_numericas.remove(target)
    
    if cols_numericas:
        resultados['univariado_cuantitativo'] = analisis_cuantitativo_completo(df, cols_numericas)
        print("‚úÖ An√°lisis cuantitativo completado")
        
        visualizar_cuantitativas(df, cols_numericas, figsize_univariado)
        print("‚úÖ Visualizaciones cuantitativas generadas")
    
    # 2. AN√ÅLISIS UNIVARIADO - CUALITATIVAS
    print(f"\nüìä AN√ÅLISIS UNIVARIADO - VARIABLES CUALITATIVAS")
    print("-" * 50)
    cols_categoricas = df.select_dtypes(include=['object', 'category']).columns.tolist()
    
    if cols_categoricas:
        resultados['univariado_cualitativo'] = analisis_cualitativo_completo(df, cols_categoricas)
        print("‚úÖ An√°lisis cualitativo completado")
        
        visualizar_cualitativas(df, cols_categoricas, figsize_univariado)
        print("‚úÖ Visualizaciones cualitativas generadas")
    
    # 3. AN√ÅLISIS MULTIVARIADO - CORRELACIONES
    if len(cols_numericas) > 1:
        print(f"\nüîó AN√ÅLISIS MULTIVARIADO - CORRELACIONES")
        print("-" * 50)
        
        resultados['correlaciones'] = resumen_correlaciones(df, cols_numericas)
        print("‚úÖ An√°lisis de correlaciones completado")
        
        visualizar_correlaciones(df, cols_numericas, figsize=figsize_multivariado)
        print("‚úÖ Visualizaciones de correlaciones generadas")
    
    # 4. COMPARACI√ìN DE GRUPOS (si hay target categ√≥rico)
    if target and target in cols_categoricas:
        print(f"\n‚öñÔ∏è COMPARACI√ìN DE GRUPOS CON TARGET: {target}")
        print("-" * 50)
        
        resultados['comparacion_grupos'] = {}
        
        for col in cols_numericas:
            resultado = comparar_grupos_cuanti(df, col, target)
            resultados['comparacion_grupos'][col] = resultado
            print(f"‚úÖ An√°lisis {col} vs {target} completado")
        
        # Visualizaciones
        for col in cols_numericas[:4]:  # Limitar a 4 para no saturar
            visualizar_grupos_cuanti(df, col, target)
    
    # 5. AN√ÅLISIS DE INDEPENDENCIA (categ√≥ricas vs categ√≥ricas)
    if len(cols_categoricas) > 1:
        print(f"\nüîó AN√ÅLISIS DE INDEPENDENCIA - VARIABLES CATEG√ìRICAS")
        print("-" * 50)
        
        resultados['independencia_categoricas'] = {}
        
        for i, var1 in enumerate(cols_categoricas):
            for var2 in cols_categoricas[i+1:]:
                if target and var2 == target:
                    resultado = analizar_independencia_categoricas(df, var1, var2)
                    resultados['independencia_categoricas'][f"{var1}_vs_{var2}"] = resultado
                    print(f"‚úÖ An√°lisis {var1} vs {var2} completado")
                    
                    visualizar_categoricas(df, var1, var2)
    
    print(f"\nüéØ PIPELINE EDA COMPLETADO")
    print("=" * 60)
    
    return resultados

def resumen_ejecutivo_eda(resultados_eda):
    """
    Genera un resumen ejecutivo de los resultados del EDA.
    
    Parameters:
    -----------
    resultados_eda : dict
        Resultados del pipeline_eda_completo()
        
    Returns:
    --------
    str : Resumen ejecutivo en formato texto
    """
    resumen = []
    resumen.append("üéØ RESUMEN EJECUTIVO - AN√ÅLISIS EXPLORATORIO DE DATOS")
    resumen.append("=" * 65)
    
    # Informaci√≥n general
    info = resultados_eda['info_general']
    resumen.append(f"\nüìä CARACTER√çSTICAS DEL DATASET:")
    resumen.append(f"   ‚Ä¢ Dimensiones: {info['dimensiones'][0]} filas √ó {info['dimensiones'][1]} columnas")
    resumen.append(f"   ‚Ä¢ Variables num√©ricas: {info['n_numericas']}")
    resumen.append(f"   ‚Ä¢ Variables categ√≥ricas: {info['n_categoricas']}")
    resumen.append(f"   ‚Ä¢ Valores faltantes: {info['valores_faltantes']}")
    
    # An√°lisis cuantitativo
    if 'univariado_cuantitativo' in resultados_eda:
        cuanti = resultados_eda['univariado_cuantitativo']
        resumen.append(f"\nüìà VARIABLES CUANTITATIVAS:")
        
        # Variables con mayor asimetr√≠a
        var_asimetrica = cuanti.loc[cuanti['Asimetr√≠a'].abs().idxmax()]
        resumen.append(f"   ‚Ä¢ Mayor asimetr√≠a: {var_asimetrica['Variable']} (skew = {var_asimetrica['Asimetr√≠a']:.3f})")
        
        # Variables con outliers
        vars_outliers = cuanti[cuanti['Outliers_IQR'] > 0]['Variable'].tolist()
        resumen.append(f"   ‚Ä¢ Variables con outliers: {len(vars_outliers)} de {len(cuanti)}")
        
        # Normalidad
        vars_normales = cuanti[cuanti['Normal_DAgostino'] == 'S√≠']['Variable'].tolist()
        resumen.append(f"   ‚Ä¢ Variables normales (D'Agostino): {len(vars_normales)} de {len(cuanti)}")
    
    # Correlaciones
    if 'correlaciones' in resultados_eda:
        corr = resultados_eda['correlaciones']
        corr_sig = corr[corr['Significativo_Œ±05'] == 'S√≠']
        corr_fuerte = corr_sig[corr_sig['Fuerza'].isin(['Fuerte', 'Muy fuerte'])]
        
        resumen.append(f"\nüîó CORRELACIONES:")
        resumen.append(f"   ‚Ä¢ Correlaciones significativas: {len(corr_sig)} de {len(corr)}")
        resumen.append(f"   ‚Ä¢ Correlaciones fuertes: {len(corr_fuerte)}")
        
        if len(corr_fuerte) > 0:
            top_corr = corr_fuerte.iloc[0]
            resumen.append(f"   ‚Ä¢ Correlaci√≥n m√°s fuerte: {top_corr['Par_Variables']} (œÅ = {top_corr.iloc[1]:.3f})")
    
    # Comparaci√≥n de grupos
    if 'comparacion_grupos' in resultados_eda:
        grupos = resultados_eda['comparacion_grupos']
        resumen.append(f"\n‚öñÔ∏è COMPARACI√ìN DE GRUPOS:")
        
        diferencias_sig = 0
        for var, resultado in grupos.items():
            if 'test_t_significativo' in resultado and resultado['test_t_significativo']:
                diferencias_sig += 1
            elif 'anova_significativo' in resultado and resultado['anova_significativo']:
                diferencias_sig += 1
        
        resumen.append(f"   ‚Ä¢ Variables con diferencias significativas: {diferencias_sig} de {len(grupos)}")
    
    # Independencia categ√≥ricas
    if 'independencia_categoricas' in resultados_eda:
        independencia = resultados_eda['independencia_categoricas']
        asociaciones_sig = sum(1 for r in independencia.values() if r['significativo'])
        
        resumen.append(f"\nüîó VARIABLES CATEG√ìRICAS:")
        resumen.append(f"   ‚Ä¢ Asociaciones significativas: {asociaciones_sig} de {len(independencia)}")
        
        if asociaciones_sig > 0:
            # Buscar asociaci√≥n m√°s fuerte
            max_cramer = 0
            max_vars = ""
            for resultado in independencia.values():
                if resultado['significativo'] and resultado['cramers_v'] > max_cramer:
                    max_cramer = resultado['cramers_v']
                    max_vars = resultado['variables']
            
            if max_vars:
                resumen.append(f"   ‚Ä¢ Asociaci√≥n m√°s fuerte: {max_vars} (Cram√©r's V = {max_cramer:.3f})")
    
    resumen.append(f"\n" + "=" * 65)
    
    return "\n".join(resumen)

# ============================================================================
# üîß FUNCIONES AUXILIARES
# ============================================================================

def calcular_bins_freedman_diaconis(serie):
    """
    Calcula el n√∫mero √≥ptimo de bins usando la regla de Freedman-Diaconis.
    
    Parameters:
    -----------
    serie : pd.Series
        Serie de datos num√©ricos
        
    Returns:
    --------
    int : N√∫mero de bins recomendado
    """
    serie_clean = serie.dropna()
    if len(serie_clean) == 0:
        return 10
    
    q75, q25 = np.percentile(serie_clean, [75, 25])
    iqr = q75 - q25
    h = 2 * iqr * (len(serie_clean) ** (-1/3))
    
    if h <= 0:
        return 10
    
    num_bins = int(np.ceil((serie_clean.max() - serie_clean.min()) / h))
    return max(1, min(num_bins, 50))  # Limitar entre 1 y 50 bins

def crear_reporte_missing_values(df):
    """
    Crea un reporte detallado de valores faltantes.
    
    Parameters:
    -----------
    df : pd.DataFrame
        DataFrame a analizar
        
    Returns:
    --------
    pd.DataFrame : Reporte de valores faltantes
    """
    missing_stats = []
    
    for col in df.columns:
        n_missing = df[col].isnull().sum()
        pct_missing = (n_missing / len(df)) * 100
        dtype = str(df[col].dtype)
        n_unique = df[col].nunique(dropna=False)
        
        missing_stats.append({
            'Columna': col,
            'Tipo': dtype,
            'N_Missing': n_missing,
            'Pct_Missing': pct_missing,
            'N_Unique': n_unique,
            'N_Valid': len(df) - n_missing
        })
    
    reporte = pd.DataFrame(missing_stats)
    reporte = reporte.sort_values('Pct_Missing', ascending=False)
    
    return reporte.round(2)

def detectar_outliers_multiple(df, columnas=None, metodos=['iqr', 'zscore', 'isolation']):
    """
    Detecci√≥n de outliers con m√∫ltiples m√©todos.
    
    Parameters:
    -----------
    df : pd.DataFrame
        DataFrame con los datos
    columnas : list, optional
        Lista de columnas num√©ricas a analizar
    metodos : list
        M√©todos de detecci√≥n ('iqr', 'zscore', 'isolation')
        
    Returns:
    --------
    dict : Resultados por m√©todo y columna
    """
    if columnas is None:
        columnas = df.select_dtypes(include=[np.number]).columns.tolist()
    
    resultados = {}
    
    for col in columnas:
        serie = df[col].dropna()
        if len(serie) == 0:
            continue
            
        resultados[col] = {}
        
        if 'iqr' in metodos:
            # M√©todo IQR
            q1, q3 = serie.quantile([0.25, 0.75])
            iqr = q3 - q1
            limite_inf = q1 - 1.5 * iqr
            limite_sup = q3 + 1.5 * iqr
            outliers_iqr = ((serie < limite_inf) | (serie > limite_sup))
            
            resultados[col]['iqr'] = {
                'n_outliers': outliers_iqr.sum(),
                'pct_outliers': (outliers_iqr.sum() / len(serie)) * 100,
                'limite_inferior': limite_inf,
                'limite_superior': limite_sup,
                'outliers_mask': outliers_iqr
            }
        
        if 'zscore' in metodos:
            # M√©todo Z-score
            z_scores = np.abs(stats.zscore(serie))
            outliers_z = z_scores > 3
            
            resultados[col]['zscore'] = {
                'n_outliers': outliers_z.sum(),
                'pct_outliers': (outliers_z.sum() / len(serie)) * 100,
                'threshold': 3,
                'outliers_mask': outliers_z
            }
        
        if 'isolation' in metodos:
            # Isolation Forest (requiere sklearn)
            try:
                from sklearn.ensemble import IsolationForest
                iso_forest = IsolationForest(contamination=0.1, random_state=42)
                outliers_iso = iso_forest.fit_predict(serie.values.reshape(-1, 1)) == -1
                
                resultados[col]['isolation'] = {
                    'n_outliers': outliers_iso.sum(),
                    'pct_outliers': (outliers_iso.sum() / len(serie)) * 100,
                    'contamination': 0.1,
                    'outliers_mask': pd.Series(outliers_iso, index=serie.index)
                }
            except ImportError:
                print("Warning: sklearn no disponible para Isolation Forest")
    
    return resultados

# ============================================================================
# üé® CONFIGURACI√ìN DE ESTILO PARA GR√ÅFICOS
# ============================================================================

def configurar_estilo_graficos():
    """
    Configura el estilo por defecto para los gr√°ficos.
    """
    plt.style.use('default')
    sns.set_palette("husl")
    
    # Configuraci√≥n de matplotlib
    plt.rcParams['figure.figsize'] = (12, 8)
    plt.rcParams['font.size'] = 10
    plt.rcParams['axes.titlesize'] = 12
    plt.rcParams['axes.labelsize'] = 10
    plt.rcParams['xtick.labelsize'] = 9
    plt.rcParams['ytick.labelsize'] = 9
    plt.rcParams['legend.fontsize'] = 9
    plt.rcParams['grid.alpha'] = 0.3

# ============================================================================
# üìù FUNCI√ìN DE AYUDA Y DOCUMENTACI√ìN
# ============================================================================

def mostrar_ayuda():
    """
    Muestra la documentaci√≥n de uso de las funciones principales.
    """
    help_text = """
üîß EDA UTILITIES - GU√çA DE USO
==============================

üìà AN√ÅLISIS UNIVARIADO:
- analisis_cuantitativo_completo(df, columnas=None)
- visualizar_cuantitativas(df, columnas=None) 
- analisis_cualitativo_completo(df, columnas=None)
- visualizar_cualitativas(df, columnas=None)

üîó AN√ÅLISIS MULTIVARIADO:
- correlaciones_completas(df, columnas=None, metodos=['spearman'])
- resumen_correlaciones(df, columnas=None, metodo='spearman')
- visualizar_correlaciones(df, columnas=None, metodo='spearman')

‚öñÔ∏è COMPARACI√ìN DE GRUPOS:
- comparar_grupos_cuanti(df, var_continua, var_categorica)
- visualizar_grupos_cuanti(df, var_continua, var_categorica)
- analizar_independencia_categoricas(df, var1, var2)
- visualizar_categoricas(df, var1, var2)

üîÑ DISCRETIZACI√ìN:
- discretizar_variable(df, columna, metodos=['equal_width', 'quantiles', 'kmeans'])
- visualizar_discretizacion(df, columna, resultados)
- comparar_discretizacion_supervivencia(df, columna, target, resultados)

üéØ PIPELINE COMPLETO:
- pipeline_eda_completo(df, target=None)
- resumen_ejecutivo_eda(resultados_eda)

üîß UTILIDADES:
- crear_reporte_missing_values(df)
- detectar_outliers_multiple(df, columnas=None)
- calcular_bins_freedman_diaconis(serie)
- configurar_estilo_graficos()

EJEMPLO DE USO:
===============
import utilities as eda

# Pipeline completo
resultados = eda.pipeline_eda_completo(df, target='Survived')

# Resumen ejecutivo
print(eda.resumen_ejecutivo_eda(resultados))

# An√°lisis espec√≠ficos
resumen_corr = eda.resumen_correlaciones(df, metodo='spearman')
eda.visualizar_correlaciones(df, metodo='spearman')
"""
    print(help_text)

if __name__ == "__main__":
    print("üîß EDA Utilities cargadas correctamente!")
    print("Usa mostrar_ayuda() para ver la documentaci√≥n.")
    configurar_estilo_graficos()